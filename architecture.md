# Mobyclaw Architecture

> **Source of truth** for all design decisions. Every significant pattern,
> trade-off, and rationale lives here. Consult before making changes; update
> after making decisions.

---

## 1. Vision

**Mobyclaw** is a long-lived personal AI agent that runs in Docker containers.
You deploy it, connect your messaging apps, and it becomes your always-on AI
companion â€” with persistent memory, a personality, and the ability to take
action on your behalf.

Think: **OpenClaw, but containerized and powered by cagent.**

### What it is

- A **personal AI agent** that's always running (long-lived daemon)
- Reachable through your **messaging apps** (Telegram, WhatsApp, Discord, Slack, etc.)
- Has **persistent memory** â€” it remembers who you are, what you've discussed, your preferences
- **Proactive** â€” it can wake itself up via heartbeats, cron jobs, and scheduled tasks
- Runs in **Docker Compose** â€” one `docker compose up` and you have a personal AI
- Powered by **cagent** â€” the agent loop, tool execution, and model inference

### What it is not

- Not a chatbot SDK â€” it's a deployable personal agent, ready to go
- Not a SaaS â€” runs on your machine, your infrastructure, your API keys
- Not stateless â€” the whole point is that it persists, remembers, and acts over time

### How it maps to OpenClaw

| OpenClaw | Mobyclaw | Notes |
|---|---|---|
| Gateway (monolithic Node.js daemon) | **Gateway container** (orchestrator) | Handles messaging, sessions, cron, heartbeat |
| Agent Loop (embedded in Gateway) | **cagent** (agent container) | cagent handles inference + tools |
| SOUL.md, IDENTITY.md, USER.md | **soul.yaml** per agent | Single file: personality + cagent config, inline `instruction` |
| MEMORY.md + memory/*.md | **Same** â€” Markdown in `~/.mobyclaw/` | Bind-mounted host dir, agent reads/writes via tools |
| HEARTBEAT.md + heartbeat runner | **Gateway scheduler** | Sends periodic prompts to agent |
| Cron jobs | **Gateway scheduler** | Persistent cron, stored in `~/.mobyclaw/` |
| Messaging channels (WA, TG, etc.) | **Gateway adapters** | Pluggable modules inside gateway, enabled via env vars |
| Sandboxing (Docker for tool isolation) | **Containers ARE the runtime** | Agent already runs in Docker |
| CLI (`openclaw gateway`, etc.) | **`mobyclaw` CLI** (bash) | Wraps docker compose |
| Sessions + command queue | **Gateway** manages sessions | Routes messages, serializes runs |
| Tool policy (allow/deny) | **cagent toolset config** | Per-agent YAML |
| Multi-agent routing + bindings | **Out of scope** â€” single agent by design | â€” |

---

## 2. Core Concepts

### 2.1 The Personal Agent

Mobyclaw runs a **personal AI agent** that:
1. Has a **personality** defined in `soul.yaml`
2. Has **persistent memory** in Markdown files (MEMORY.md, memory/*.md)
3. Is **always running** in a Docker container
4. Can be **reached** via messaging apps or direct HTTP/CLI
5. Can **wake itself** via heartbeats and cron jobs
6. Can **take action** using tools (shell, filesystem, fetch, etc.)

### 2.2 Moby â€” The One Agent

Mobyclaw runs **exactly one agent: moby**. There is no multi-agent routing,
no agent marketplace, no agent registry. One agent, one container, one personality.
This is a deliberate simplification â€” a personal AI agent doesn't need to be a
platform.

Moby lives at `agents/moby/` and has:
- `soul.yaml` â€” Moby's personality, model, tools, and behavioral guidelines (all-in-one)

### 2.3 Trigger Sources

The agent doesn't just respond to messages. It has multiple input sources:

| Trigger | How it works | Example |
|---|---|---|
| **Messaging** | User sends a message via WhatsApp/Telegram/etc. | "What's on my calendar today?" |
| **Heartbeat** | Periodic wake-up (every 30m by default) | Agent checks HEARTBEAT.md, reviews pending tasks |
| **Cron** | Scheduled jobs (one-shot or recurring) | "Every morning at 7am, summarize my emails" |
| **Webhook** | HTTP POST to the gateway | GitHub push event triggers code review |
| **CLI** | Direct prompt via `mobyclaw run` | `mobyclaw run "Deploy to staging"` |

### 2.4 Memory

Memory is **plain Markdown files on the host** at `~/.mobyclaw/`, bind-mounted
into containers. Same philosophy as OpenClaw.

| File | Purpose | Lifecycle |
|---|---|---|
| `MEMORY.md` | Curated long-term memory (facts, preferences, people) | Agent maintains it |
| `memory/YYYY-MM-DD.md` | Daily log (append-only notes) | One per day, auto-created |
| `HEARTBEAT.md` | Heartbeat checklist (what to check each wake-up) | User/agent maintained |
| `soul.yaml` | Agent personality + config (user-editable!) | User/agent maintained |

These files live at `~/.mobyclaw/` on the host. Users can edit `soul.yaml` or
`HEARTBEAT.md` directly with any text editor â€” the agent picks up changes on
the next turn.

**Phase 2+**: Vector search over memory files for semantic recall.

### 2.5 User Data Directory

All evolving agent state lives on the host filesystem, not inside Docker
volumes. This makes it visible, editable, and portable.

```
~/.mobyclaw/
â”œâ”€â”€ soul.yaml            # Agent personality + config (user-editable)
â”œâ”€â”€ MEMORY.md            # Long-term curated memory
â”œâ”€â”€ TASKS.md             # Agent's task/reminder list
â”œâ”€â”€ HEARTBEAT.md         # Heartbeat checklist
â”œâ”€â”€ schedules.json       # Scheduled reminders (gateway-managed)
â”œâ”€â”€ credentials.env      # Service credentials (GH_TOKEN, AWS keys, etc.)
â”œâ”€â”€ workspaces.conf      # Workspace folder mappings (name=path)
â”œâ”€â”€ memory/              # Daily logs
â”‚   â”œâ”€â”€ 2026-02-23.md
â”‚   â”œâ”€â”€ 2026-02-22.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ sessions/            # Conversation history
â””â”€â”€ logs/                # Agent activity logs
```

**Platform paths:**

| OS | Path |
|---|---|
| macOS | `~/.mobyclaw/` |
| Linux | `~/.mobyclaw/` |
| Windows | `%USERPROFILE%\.mobyclaw\` |

**Why bind mount, not Docker volume?**
- Users can see and edit files directly (`vim ~/.mobyclaw/soul.yaml`)
- Easy to back up, sync, or version control
- Survives `docker system prune` â€” Docker volumes don't
- Portable: copy `~/.mobyclaw/` to a new machine and your agent comes with you

### 2.6 Workspaces

Workspaces are **host directories bind-mounted into the agent container** at
`/workspace/<name>`. This lets the agent read, modify, and create files in the
user's actual projects.

- **Configured in** `~/.mobyclaw/workspaces.conf` (simple `name=path` format)
- **Mounted via** `docker-compose.override.yml` (auto-generated, see Â§8.3)
- **Managed via** `mobyclaw workspace add|remove|list` CLI commands
- **Available at** `/workspace/<name>` inside the agent container

Workspaces are real bind mounts â€” changes are bidirectional and immediate.
The agent's `soul.yaml` instructions tell it to check `/workspace/` when the
user asks to work on "their project" or "their code".

### 2.7 Service Credentials

Service credentials let the agent use external tools (GitHub CLI, AWS CLI, etc.)
on the user's behalf. They are **environment variables injected into the agent
container** at runtime.

- **Configured in** `~/.mobyclaw/credentials.env` (standard `KEY=value` format)
- **Injected via** `docker-compose.override.yml` `env_file` directive
- **Managed via** `mobyclaw init` (interactive) or direct file editing
- **Never exposed** â€” the agent's instructions prohibit displaying credential values

Credentials are separate from `.env` because they serve different purposes:
- `.env` = mobyclaw infrastructure (LLM keys, messaging tokens, settings)
- `credentials.env` = user's service tokens (passed through to the agent)

This separation means `.env` stays in the project root (gitignored) while
`credentials.env` lives in `~/.mobyclaw/` alongside the agent's other state.

### 2.8 Sessions

A session is a conversation thread with history. Sessions are:
- **Per-channel**: each DM, group, or platform gets its own session
- **Persistent**: stored on disk, survive container restarts
- **Compactable**: old context gets summarized to stay within model limits

### 2.9 Self-Modification

Moby can **modify its own configuration** and trigger a restart to load
the changes. This enables the agent to evolve its own personality,
switch models, or adjust behavior based on user feedback.

**Mechanism:** File-signal pattern.

```
Agent edits ~/.mobyclaw/soul.yaml
  â”‚
  â”œâ”€ echo "restart" > ~/.mobyclaw/.restart
  â”‚
  â–¼
Host-side watcher (spawned by `mobyclaw up`)
  â”‚
  â”œâ”€ Polls every 5 seconds
  â”œâ”€ Sees .restart file
  â”œâ”€ Reads action: "restart" or "rebuild"
  â”œâ”€ Removes file
  â”œâ”€ docker compose restart moby   (config changes, ~5s)
  â”‚   OR docker compose up --build  (package changes, ~30s)
  â””â”€ Logs to ~/.mobyclaw/logs/watcher.log
```

**Why a file signal, not an API or Docker socket?**
- No Docker socket inside containers (security)
- No new dependencies (just a file + poll loop)
- Works on any platform
- The CLI already knows how to run docker compose
- The file is in the bind-mounted directory â€” both host and container can see it

**Watcher lifecycle:**
- Spawned as a background process by `mobyclaw up`
- PID stored in `~/.mobyclaw/.watcher.pid`
- Killed by `mobyclaw down`
- Idempotent â€” `mobyclaw up` kills any existing watcher before spawning a new one

**cagent does NOT hot-reload soul.yaml.** Confirmed by testing: the instruction
is read once at process start. A container restart is required for config
changes to take effect. Memory files (MEMORY.md, TASKS.md) are unaffected
by restarts since they're bind-mounted from the host.

---

## 3. Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Host Machine                                 â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚ mobyclaw   â”‚â”€â”€ docker compose up/down/logs/run â”€â”€â”            â”‚
â”‚  â”‚ CLI        â”‚                                       â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â–¼            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  Docker Compose Stack                         â”‚  â”‚
â”‚  â”‚                  (mobyclaw network)                           â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚                    gateway                             â”‚   â”‚  â”‚
â”‚  â”‚  â”‚              (orchestrator container)                   â”‚   â”‚  â”‚
â”‚  â”‚  â”‚                                                         â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  Messaging  â”‚ â”‚ Session  â”‚ â”‚     Scheduler        â”‚ â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  â”‚  Adapters   â”‚ â”‚ Store    â”‚ â”‚  (Heartbeat + Cron)  â”‚ â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ TG/WA/DC/SLâ”‚ â”‚          â”‚ â”‚                      â”‚ â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚  â”‚
â”‚  â”‚  â”‚         â”‚                                â”‚              â”‚   â”‚  â”‚
â”‚  â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚  â”‚
â”‚  â”‚  â”‚                      â”‚                                   â”‚   â”‚  â”‚
â”‚  â”‚  â”‚              HTTP to agent container                     â”‚   â”‚  â”‚
â”‚  â”‚  â”‚                      â”‚                                   â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  :3000 (control API) â”‚                                   â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚                         â”‚                                     â”‚  â”‚
â”‚  â”‚                         â–¼                                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚                     moby                               â”‚   â”‚  â”‚
â”‚  â”‚  â”‚              (agent container)                          â”‚   â”‚  â”‚
â”‚  â”‚  â”‚         cagent serve api soul.yaml                     â”‚   â”‚  â”‚
â”‚  â”‚  â”‚                                                         â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  instruction â”‚ tools â”‚ memory r/w â”‚ workspace access    â”‚   â”‚  â”‚
â”‚  â”‚  â”‚                                                         â”‚   â”‚  â”‚
â”‚  â”‚  â”‚  :8080 (cagent HTTP API)                               â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚             â”‚                                â”‚                 â”‚  â”‚
â”‚  â”‚        MCP stdio                        MCP stdio              â”‚  â”‚
â”‚  â”‚             â”‚                                â”‚                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚
â”‚  â”‚  â”‚     workspace       â”‚     â”‚        memory            â”‚     â”‚  â”‚
â”‚  â”‚  â”‚   (MCP server)      â”‚     â”‚     (MCP server)         â”‚     â”‚  â”‚
â”‚  â”‚  â”‚                     â”‚     â”‚                          â”‚     â”‚  â”‚
â”‚  â”‚  â”‚  mounts host dirs   â”‚     â”‚  ~/.mobyclaw/ bind mount â”‚     â”‚  â”‚
â”‚  â”‚  â”‚  at /mnt/host       â”‚     â”‚  MEMORY.md, memory/,     â”‚     â”‚  â”‚
â”‚  â”‚  â”‚                     â”‚     â”‚  soul.yaml, HEARTBEAT.md  â”‚     â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚
â”‚  â”‚                                                               â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚  â”‚
â”‚  â”‚  â”‚  /data           â”‚ (Docker volume â€” gateway only)          â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€ sessions/   â”‚                                         â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€ cron/       â”‚                                         â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€ state/      â”‚                                         â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Container Roles

The stack is built from **4 core services**, each owning one concern:

| Container | Role | Technology | Phase |
|---|---|---|---|
| **moby** | AI brain â€” runs cagent, receives prompts, calls tools on other services | cagent serve api | 1 |
| **gateway** | Orchestrator â€” messaging adapters, sessions, heartbeat, cron | Node.js or Go | 2 |
| **workspace** | Filesystem â€” mounts host directories, exposes them as MCP tools to the agent | MCP server (cagent serve mcp) | 1 |
| **memory** | Memory â€” stores MEMORY.md + daily logs, provides vector search | MCP server + search API | 1 |

Messaging platforms are **adapters inside the gateway**, not separate containers:

| Adapter | Library | Enabled via |
|---|---|---|
| Telegram | Telegraf | `TELEGRAM_BOT_TOKEN` env var |
| WhatsApp | Baileys / whatsapp-web.js | `WHATSAPP_AUTH` env var |
| Discord | discord.js | `DISCORD_BOT_TOKEN` env var |
| Slack | Bolt | `SLACK_BOT_TOKEN` env var |

**Why adapters inside gateway, not separate bridge containers?**
- Simpler: one container, one codebase, one config
- All messaging libraries are Node.js anyway
- Separate containers = more images, more networking, more config for little benefit
- OpenClaw does it this way â€” all channels live in the gateway
- Enable/disable via env var presence: no token = adapter doesn't load

### How Services Connect

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  Telegram, WA,     â”‚  gateway  â”‚  messaging, cron, heartbeat
  Discord, Slack â”€â†’ â”‚  :3000    â”‚
                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                          â”‚ HTTP
                          â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   moby    â”‚  AI brain (cagent serve api)
                    â”‚  :8080    â”‚
                    â””â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
              MCP stdioâ”‚     â”‚MCP stdio
                       â–¼     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚workspace â”‚ â”‚  memory  â”‚
              â”‚(MCP srv) â”‚ â”‚(MCP srv) â”‚
              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                   â”‚            â”‚
              host mounts   ~/.mobyclaw/
```

**Connection protocols:**

| From â†’ To | Protocol | How |
|---|---|---|
| gateway â†’ moby | HTTP | POST to cagent's `/v1/run` API |
| moby â†’ workspace | MCP (stdio) | cagent `type: mcp` toolset, `command` launches client that connects to workspace container |
| moby â†’ memory | MCP (stdio) | cagent `type: mcp` toolset, `command` launches client that connects to memory container |
| CLI â†’ moby | HTTP | Direct `curl` to cagent API (Phase 1, no gateway) |

### Runtime Modes (cagent)

cagent supports multiple serving modes. We use:

| Mode | Command | Use Case |
|---|---|---|
| **API Server** | `cagent serve api soul.yaml` | Primary: HTTP API for agent interaction |
| **A2A Server** | `cagent serve a2a soul.yaml` | Future: Agent-to-agent protocol |
| **Exec** | `cagent run --exec soul.yaml` | One-shot: run a task and exit |
| **Interactive** | `cagent run soul.yaml` | Dev/debug: TUI inside container |

---

## 4. Project Structure

```
mobyclaw/
â”œâ”€â”€ architecture.md            # This file â€” source of truth
â”œâ”€â”€ mobyclaw.yaml              # DEV ONLY: cagent config for the development agent
â”‚
â”œâ”€â”€ agents/                    # Agent definitions
â”‚   â””â”€â”€ moby/                  # The default "moby" agent
â”‚       â”œâ”€â”€ soul.yaml          # All-in-one: personality, model, tools, behavior
â”‚       â””â”€â”€ defaults/          # Default files copied to ~/.mobyclaw/ on init
â”‚           â”œâ”€â”€ MEMORY.md      # Initial memory template
â”‚           â”œâ”€â”€ HEARTBEAT.md   # Initial heartbeat checklist
â”‚           â”œâ”€â”€ credentials.env # Credential file template (comments only)
â”‚           â””â”€â”€ workspaces.conf # Workspace config template (comments only)
â”‚
â”œâ”€â”€ gateway/                   # Gateway orchestrator
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ index.js           # Express app, /prompt and /prompt/stream endpoints
â”‚       â”œâ”€â”€ agent-client.js    # HTTP client for cagent API with SSE streaming
â”‚       â”œâ”€â”€ sessions.js        # Session store with per-channel queuing
â”‚       â”œâ”€â”€ scheduler.js       # Schedule store, scheduler loop, heartbeat timer
â”‚       â”œâ”€â”€ tool-labels.js     # Shared tool name â†’ human-readable label formatting
â”‚       â””â”€â”€ adapters/          # Messaging platform adapters
â”‚           â””â”€â”€ telegram.js    # Telegraf bot with progressive message editing
â”‚
â”œâ”€â”€ Dockerfile                 # Agent base image: Debian + cagent + tools
â”œâ”€â”€ docker-compose.yml         # Static compose manifest (git-committed)
â”œâ”€â”€ docker-compose.override.yml # GENERATED: credentials + workspace mounts (gitignored)
â”œâ”€â”€ .env.example               # Template for API keys and config
â”œâ”€â”€ .env                       # Actual secrets (gitignored, created by init)
â”‚
â”œâ”€â”€ mobyclaw                    # CLI script (bash)
â”‚
â””â”€â”€ README.md                  # User-facing documentation
```

### What's NOT in the product

- `mobyclaw.yaml` â€” This is the cagent config for the **development agent** that
  helps build mobyclaw. It is not part of the product runtime.

---

## 5. Agent Definition Format

### 5.1 Agent Config (`agents/<name>/soul.yaml`)

This is a **standard cagent agent YAML** with the full personality inlined.
Example for moby:

```yaml
agents:
  root:
    name: moby
    model: opus

    instruction: |
      # Moby â€” Your Personal AI Agent

      You are **Moby**, a personal AI agent running in a Docker container...

      ## Identity
      - **Name:** Moby
      - **Tone:** Conversational but precise...

      ## Memory
      ...

      ## Constraints
      ...

    toolsets:
      - type: shell
      - type: filesystem
      - type: fetch
      - type: think

    add_date: true
    add_environment_info: true
```

**Design decision:** We use cagent's native YAML format directly. No wrapper,
no abstraction. This means:
- Zero translation layer between mobyclaw config and cagent config
- Users can leverage any cagent feature without mobyclaw needing to know about it
- cagent docs apply directly

The personality lives inside the `instruction:` field as a YAML block scalar (`|`).
This keeps everything in one file while remaining readable â€” the `instruction`
block is effectively Markdown inside YAML.

**Why not a separate soul.md?** cagent's `instruction` field is string-only â€”
it doesn't support `file:` references. While `add_prompt_files` can inject file
contents into the prompt, having the personality inline means:
- One file to understand the whole agent
- One file to copy to a new machine
- One file to edit when customizing
- No hidden dependencies between files

### 5.2 Runtime File (`~/.mobyclaw/soul.yaml`)

At runtime, the agent's `soul.yaml` is loaded from `~/.mobyclaw/soul.yaml` (the
user's copy), not from the repo. On first run, `mobyclaw init` copies the
default `agents/moby/soul.yaml` to `~/.mobyclaw/soul.yaml` as a starting point.

```
~/.mobyclaw/
â”œâ”€â”€ soul.yaml           # Active agent config (user-editable)
â”œâ”€â”€ MEMORY.md           # Long-term curated memory
â”œâ”€â”€ TASKS.md            # Agent's task/reminder list
â”œâ”€â”€ HEARTBEAT.md        # Heartbeat checklist
â”œâ”€â”€ schedules.json      # Scheduled reminders (gateway-managed)
â”œâ”€â”€ credentials.env     # Service credentials (GH_TOKEN, etc.)
â”œâ”€â”€ workspaces.conf     # Workspace folder mappings
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ 2026-02-23.md   # Daily log
â”‚   â””â”€â”€ ...
â”œâ”€â”€ sessions/           # Conversation history
â””â”€â”€ logs/               # Agent logs
```

These are just files on the host. The agent reads and writes them via
cagent's built-in filesystem tools. They persist across container restarts
because they're bind-mounted from the host filesystem.

---

## 6. Gateway (Orchestrator)

The gateway is the **central nervous system** of mobyclaw. It's a long-lived
process that:

1. **Receives messages** from all connected channels (Telegram, WhatsApp, CLI, webhooks)
2. **Manages sessions** â€” maps channels/users to conversation threads
3. **Routes to the agent** â€” sends prompts to cagent's HTTP API
4. **Runs the scheduler** â€” heartbeats and cron jobs trigger agent turns
5. **Delivers responses** â€” routes agent replies back to the right channel

### 6.1 Message Flow

```
User sends "What's my schedule?" via Telegram
  â”‚
  â–¼
gateway's Telegram adapter receives message
  â”‚
  â”œâ”€ Look up session for telegram:dm:12345
  â”œâ”€ Load session history
  â”œâ”€ Enqueue in command queue (serialize per session)
  â”‚
  â–¼
gateway sends agent turn
  â”‚
  â”œâ”€ POST http://moby:8080/v1/run
  â”‚   { prompt: "What's my schedule?", session_id: "..." }
  â”‚
  â–¼
cagent runs agent loop
  â”‚
  â”œâ”€ Assembles system prompt (soul.yaml instruction + context)
  â”œâ”€ Model inference
  â”œâ”€ Tool calls (reads calendar, writes memory, etc.)
  â”œâ”€ Final response: "You have a standup at 10am and..."
  â”‚
  â–¼
gateway receives response
  â”‚
  â”œâ”€ Store in session history
  â”œâ”€ Route back to originating channel
  â”‚
  â–¼
gateway delivers response via Telegram adapter
```

### 6.2 Heartbeat Flow

```
Scheduler timer fires (every 30 minutes)
  â”‚
  â”œâ”€ Is it within active hours? (e.g., 8am-11pm)
  â”‚
  â–¼
gateway sends heartbeat prompt to agent
  â”‚
  â”œâ”€ POST http://moby:8080/v1/run
  â”‚   { prompt: "Read HEARTBEAT.md. Follow it strictly.
  â”‚              If nothing needs attention, reply HEARTBEAT_OK.",
  â”‚     session_id: "heartbeat:main" }
  â”‚
  â–¼
cagent runs agent loop
  â”‚
  â”œâ”€ Reads HEARTBEAT.md
  â”œâ”€ Checks pending tasks, reviews memory
  â”œâ”€ Either: "HEARTBEAT_OK" (nothing to do)
  â”‚   Or: "Reminder: you have a meeting in 30 minutes"
  â”‚
  â–¼
gateway processes response
  â”‚
  â”œâ”€ If HEARTBEAT_OK â†’ suppress, don't deliver
  â””â”€ If actual content â†’ deliver to user's last active channel
```

See Â§6.7 for the full heartbeat design.

### 6.3 Cron Flow

```
Cron job fires: "Morning brief" (every day at 7am)
  â”‚
  â–¼
gateway creates isolated session
  â”‚
  â”œâ”€ POST http://moby:8080/v1/run
  â”‚   { prompt: "Summarize overnight updates. Check emails and calendar.",
  â”‚     session_id: "cron:morning-brief" }
  â”‚
  â–¼
cagent runs agent loop
  â”‚
  â”œâ”€ Reviews overnight activity, memory, etc.
  â”œâ”€ Composes summary
  â”‚
  â–¼
gateway delivers to configured channel
  â”‚
  â””â”€ Sends summary to user's WhatsApp/Telegram/Slack
```

### 6.4 Message Serialization

cagent can only process one request per session at a time. If the gateway
sends a second message to the same session while the first is still running,
the second request will hang until the first completes (or time out).

The gateway serializes messages per channel:
- Each channel has a **queue** of pending messages
- While a message is being processed (session is "busy"), new messages are queued
- When processing completes, the next queued message is sent
- If a session error occurs, the session is reset and the message retried once

This prevents concurrent requests to the same cagent session and ensures
messages are processed in order.

### 6.5 Streaming Architecture

cagent's SSE stream emits `agent_choice` tokens as the model generates them.
The gateway streams these tokens through to all consumers in real-time,
making the agent feel fast even for long responses.

**Streaming pipeline:**

```
cagent SSE stream
  â”‚
  â”‚  agent_choice tokens (1-2s after request)
  â–¼
agent-client.js (promptStream)
  â”‚
  â”‚  onToken(text) callback
  â–¼
gateway routing (sendToAgentStream)
  â”‚
  â”œâ”€â†’ POST /prompt/stream (SSE)  â†’ CLI prints tokens to terminal
  â”œâ”€â†’ Telegram adapter           â†’ edits message every ~1s
  â””â”€â†’ POST /prompt (buffered)    â†’ waits for full response (legacy)
```

**Gateway SSE endpoint** (`POST /prompt/stream`):
- Returns `text/event-stream` with events: `token`, `tool`, `done`, `error`
- Uses a `PassThrough` stream piped to the HTTP response
- Critical: disconnect detection uses `res.on('close')`, NOT `req.on('close')`
  (the request close event fires immediately when the POST body is consumed,
  not when the client disconnects â€” this was a subtle bug)

**Telegram streaming**: Instead of waiting for the full response, the adapter:
1. Sends a placeholder message as soon as the first token arrives (~1-2s)
2. Edits that message every ~1.2s with accumulated text
3. Shows tool status ("â³ Writing to memory...") during tool calls
4. Does a final edit when the stream completes

**CLI streaming**: `mobyclaw run` and `mobyclaw chat` connect to the SSE
endpoint and print tokens directly to stdout as they arrive. Tool call
status is shown on stderr so it doesn't pollute piped output.

### 6.6 Scheduler â€” Timed Reminders & Recurring Schedules

The scheduler is a **gateway-side timer loop** that delivers pre-composed
messages at exact times. It does NOT involve the agent at delivery time â€”
the agent composes the message upfront when creating the schedule.

#### Schedule API

The gateway exposes REST endpoints for schedule management. The agent
calls these via `curl` (shell tool). The CLI and external tools can also
use them.

| Endpoint | Method | Purpose |
|---|---|---|
| `GET /api/schedules` | List | Returns pending schedules |
| `POST /api/schedules` | Create | Creates a new schedule |
| `DELETE /api/schedules/:id` | Cancel | Cancels a pending schedule |

**Create request body:**
```json
{
  "due": "2026-02-24T09:00:00Z",
  "message": "ğŸ”” Hey! Reminder: **Buy groceries!**",
  "channel": "telegram:123456",
  "repeat": null
}
```

Either `message` or `prompt` is required (or both):

| Field | When to use | At fire time |
|---|---|---|
| `message` | Simple reminders (content known upfront) | Delivered directly (free, instant) |
| `prompt` | Needs live data/reasoning (news, weather, summaries) | Sent to agent; agentâ€™s response delivered |
| Both | Prompt-based with fallback | Agent runs; if it fails, `message` is delivered |

**Prompt-based example** (agent runs at fire time):
```json
{
  "due": "2026-02-24T09:00:00Z",
  "prompt": "Fetch the latest tech news and write a brief morning briefing.",
  "channel": "telegram:123456",
  "repeat": "weekdays"
}
```

**Schedule object (stored):**
```json
{
  "id": "sch_a1b2c3",
  "due": "2026-02-24T09:00:00Z",
  "message": "ğŸ”” Hey! Reminder: **Buy groceries!**",
  "channel": "telegram:123456",
  "status": "pending",
  "repeat": null,
  "created_at": "2026-02-23T20:15:00Z",
  "delivered_at": null
}
```

**Status values:** `pending` â†’ `delivered` | `cancelled`

**Persistence:** `~/.mobyclaw/schedules.json` â€” bind-mounted, survives
restarts, user-visible. Gateway reads/writes this file.

#### Repeat / Recurring Schedules

The `repeat` field controls recurrence:

| Value | Meaning | Example |
|---|---|---|
| `null` | One-shot (default) | "Remind me tomorrow at 9am" |
| `"daily"` | Every day at the same time | "Remind me every day at 9am" |
| `"weekdays"` | Monâ€“Fri at the same time | "Every weekday morning" |
| `"weekly"` | Same day+time each week | "Every Monday at 9am" |
| `"monthly"` | Same day+time each month | "First of every month" |
| `"0 7 * * 1-5"` | Cron expression | Full cron flexibility |

When a recurring schedule fires:
1. Gateway delivers the message
2. Marks current entry as `delivered`
3. Computes next occurrence from the `repeat` rule
4. Creates a new `pending` entry with the next `due` time

The original entry's `repeat` value is copied to the new entry, creating
an ongoing chain. Cancelling the latest pending entry stops the chain.

#### Scheduler Loop

Runs every **30 seconds** inside the gateway:

```
Every 30 seconds:
  â”‚
  â”œâ”€ Read schedules.json
  â”œâ”€ Find entries where due <= now AND status == "pending"
  â”‚
  â”œâ”€ For each due schedule:
  â”‚   â”œâ”€ Parse channel (e.g., "telegram:123456")
  â”‚   â”œâ”€ Call adapter's send function via delivery API
  â”‚   â”œâ”€ Mark status = "delivered", set delivered_at
  â”‚   â”œâ”€ If repeat: create next pending entry
  â”‚   â””â”€ Save schedules.json
  â”‚
  â””â”€ Done (< 1ms for most runs)
```

#### Delivery API

Internal gateway endpoint for sending proactive messages to any channel:

```
POST /api/deliver
{
  "channel": "telegram:123456",
  "message": "ğŸ”” Reminder text"
}
```

- Parses the channel prefix (`telegram`, `discord`, `slack`, etc.)
- Routes to the appropriate adapter's proactive send function
- Returns success/failure
- Bypasses session management â€” this is a direct push, not an agent turn

**Adapter registry:** Gateway maintains a map of platform â†’ send function.
Each adapter registers itself on startup:

```js
const adapters = {
  telegram: { send: (chatId, message) => bot.telegram.sendMessage(chatId, message) },
  // discord: { send: ... },
  // slack: { send: ... },
};
```

#### How the Agent Creates a Schedule

When the user says "remind me tomorrow at 9am to buy groceries":

```
User (Telegram): "Remind me tomorrow at 9am to buy groceries"
  â”‚
  â”œâ”€ Gateway prepends channel context (see Â§6.8)
  â”‚
  â–¼
Agent processes message
  â”‚
  â”œâ”€ 1. Create schedule via gateway API:
  â”‚     curl -s -X POST http://gateway:3000/api/schedules \
  â”‚       -H "Content-Type: application/json" \
  â”‚       -d '{"due":"2026-02-24T09:00:00Z",
  â”‚            "message":"ğŸ”” Hey! Reminder: Buy groceries!",
  â”‚            "channel":"telegram:123456"}'
  â”‚
  â”œâ”€ 2. Write to TASKS.md for tracking:
  â”‚     "- [ ] 2026-02-24 09:00 â€” Buy groceries [scheduled]"
  â”‚
  â””â”€ 3. Respond: "Got it! I'll remind you tomorrow at 9am. âœ…"
```

### 6.7 Heartbeat â€” Periodic Agent Wake-Up

The heartbeat is an **intelligent periodic check** where the agent wakes
up, reviews its state, and acts if needed. Unlike the scheduler (dumb
timer, pre-composed message), the heartbeat involves full LLM reasoning.

**Trigger:** Gateway timer, every `MOBYCLAW_HEARTBEAT_INTERVAL` (default: 15m)

**Active hours:** Only fires between `MOBYCLAW_ACTIVE_HOURS` (default:
`07:00-23:00`). Silent outside these hours. Scheduled reminders always
fire regardless of active hours.

**Heartbeat prompt (sent by gateway to agent):**

```
[HEARTBEAT | time=2026-02-24T09:03:00Z]
You are being woken by a scheduled heartbeat.

1. Read TASKS.md â€” review your task list, note anything relevant
2. Read HEARTBEAT.md â€” follow the checklist
3. If you need to notify the user about something, use:
   curl -s -X POST http://gateway:3000/api/deliver \
     -H "Content-Type: application/json" \
     -d '{"channel": "CHANNEL_ID", "message": "YOUR MESSAGE"}'
4. If nothing needs attention, reply exactly: HEARTBEAT_OK
```

**Heartbeat flow:**

```
Gateway timer fires (every 15 minutes)
  â”‚
  â”œâ”€ Check active hours (07:00-23:00) â†’ skip if outside
  â”‚
  â”œâ”€ Send heartbeat prompt to agent (session: "heartbeat:main")
  â”‚
  â–¼
Agent processes heartbeat
  â”‚
  â”œâ”€ Reads TASKS.md
  â”‚   â”œâ”€ Reviews open tasks
  â”‚   â”œâ”€ Marks completed items
  â”‚   â””â”€ Cleans up old entries
  â”‚
  â”œâ”€ Reads HEARTBEAT.md
  â”‚   â”œâ”€ Follows checklist items
  â”‚   â””â”€ Daily tasks (once per day)
  â”‚
  â”œâ”€ If something needs user attention:
  â”‚   â””â”€ curl POST http://gateway:3000/api/deliver ...
  â”‚
  â””â”€ Response:
      â”œâ”€ "HEARTBEAT_OK" â†’ gateway suppresses, logs quietly
      â””â”€ Summary text â†’ gateway logs it
```

**Why the agent uses `/api/deliver` instead of just responding:**
The heartbeat runs on a system session (`heartbeat:main`), not a user
channel. The agent's response goes nowhere useful. For the agent to
reach the user, it explicitly calls the delivery API with the target
channel. This gives the agent control over WHERE to send (different
tasks may target different channels).

### 6.8 Channel Context Injection

For the agent to know which channel a message came from (needed when
creating schedules), the gateway prepends a context line to every user
message:

```
[context: channel=telegram:123456, time=2026-02-23T20:15:00Z]
Remind me tomorrow at 9am to buy groceries
```

The agent's instruction tells it to:
- Extract the channel ID when creating schedules or timed tasks
- Include the channel in schedule API calls and TASKS.md entries
- Never display the context line to the user
- Ask the user which channel to use if they request a reminder from
  a non-messaging channel (e.g., CLI) and multiple channels are available

For heartbeat prompts, no channel context is included (it's a system
session, not a user message).

**Why in the message, not metadata?** cagent's API doesn't support
per-message metadata fields. The user message content is the only field
we control. A bracketed prefix is simple, reliable, and the LLM easily
parses it.

### 6.9 TASKS.md â€” Agent's Task Store

`TASKS.md` lives at `~/.mobyclaw/TASKS.md`. It's a Markdown file the
agent uses to track reminders, todos, and recurring tasks.

```markdown
# Tasks

> Moby's task and reminder list. Moby manages this file.
> You can also edit it directly.

## Reminders

- [ ] 2026-02-24 09:00 â€” Buy groceries (channel:telegram:123456) [scheduled]
- [ ] 2026-02-24 14:00 â€” Call the dentist (channel:telegram:123456) [scheduled]
- [x] ~~2026-02-23 15:00 â€” Send report to Alice~~ (delivered)

## Recurring

- [ ] weekdays 07:00 â€” Morning briefing (channel:telegram:123456) [scheduled]

## Todo

- [ ] Review PR #1234 on myapp
- [ ] Research vector databases for memory search
- [x] ~~Set up workspace mounts~~
```

**Design:**
- Flexible Markdown â€” agent uses LLM intelligence to interpret
- `[scheduled]` marker â€” indicates a gateway schedule was created
  (prevents double-scheduling on heartbeat)
- Channel stored per-task â€” reminders go back to the originating channel
- Todos without times â€” just tracked, agent mentions in heartbeat if relevant
- Agent marks `[x]` when done, may clean up old entries

### 6.10 Last Active Channel

The gateway tracks the **last messaging channel** the user interacted with.
This is used as the default target when:
- The heartbeat needs to notify the user about something general
- A schedule was created without an explicit channel

Stored in memory (resets on gateway restart). Updated whenever a message
arrives from any messaging adapter (Telegram, Discord, etc.). CLI/API
channels do NOT update last active (they're ephemeral).

---


```
Debian slim + cagent binary + common dev tools (git, curl, jq, etc.)
```

**Design decisions:**
- **Debian slim** over Alpine: better compatibility with cagent and dev tools
- **cagent installed at build time**: pinned version for reproducibility
- **Common tools included**: git, curl, jq, ripgrep â€” agents need these for
  shell tool execution
- **Non-root user**: agent runs as `agent` user (uid 1000) for security
- **Workspace at `/workspace`**: standard mount point for all agents

### 7.2 Agent Entrypoint

```bash
cagent serve api /agent/soul.yaml --working-dir /workspace
```

The container:
1. Starts cagent in API server mode
2. Loads the agent config from `/agent/soul.yaml`
3. Sets the working directory to `/workspace` (mounted from host)
4. Listens on port 8080
5. Serves the agent API (send prompts, get responses, manage sessions)

**Tool approval:** `cagent serve api` requires explicit tool approval per
session. When creating a session via `POST /api/sessions`, the gateway MUST
set `{"tools_approved": true}` in the request body. Without this, the SSE
stream will pause at `tool_call_confirmation` events and wait indefinitely
for client-side approval that never comes. This was a critical bug discovered
during development â€” the agent would respond to simple messages (no tools)
but hang forever on any message that triggered a tool call (e.g., writing
to memory). The fix is a single field on session creation.

### 7.3 cagent HTTP API Reference

Discovered through testing. This is the API surface of `cagent serve api`:

| Endpoint | Method | Purpose |
|---|---|---|
| `/api/ping` | GET | Health check. Returns `{"status":"ok"}` |
| `/api/agents` | GET | List available agents. Returns `[{"name":"soul",...}]` |
| `/api/sessions` | GET | List all sessions |
| `/api/sessions` | POST | Create session. Body: `{"tools_approved": true}`. Returns session object with `id`. |
| `/api/sessions/{id}` | GET | Get session details and message history |
| `/api/sessions/{id}/agent/{name}` | POST | Send messages to agent. Body: `[{"role":"user","content":"..."}]`. Returns SSE stream. |

**Agent name resolution:** The `{name}` in the agent endpoint comes from the
**config filename** (e.g., `soul.yaml` â†’ agent name is `soul`), NOT from the
`name:` field in the YAML or the agents map key. This is a cagent convention.

**SSE stream event types:**

| Event Type | When | Contains |
|---|---|---|
| `agent_info` | Start of stream | Agent name, model, welcome message |
| `team_info` | Start of stream | Available agents list |
| `toolset_info` | Start of stream | Number of available tools |
| `stream_started` | Agent begins processing | Session ID |
| `agent_choice_reasoning` | During inference (thinking) | Reasoning text (extended thinking) |
| `agent_choice` | During inference | **Response text tokens** â€” this is the actual reply |
| `partial_tool_call` | Tool being called | Tool name and partial arguments (streaming) |
| `tool_call_confirmation` | Tool awaiting approval | Only if `tools_approved: false` â€” **blocks stream** |
| `tool_result` | After tool execution | Tool output |
| `message_added` | Message persisted | Session ID |
| `token_usage` | After each model turn | Input/output tokens, cost |
| `session_title` | Auto-generated | Session title from content |
| `stream_stopped` | End of stream | Session ID |
| `error` | On failure | Error message |

**Multi-turn tool streams:** A single SSE stream may contain multiple model
turns. When the model calls a tool, the stream continues through:
`agent_choice_reasoning` â†’ `partial_tool_call` â†’ (tool executes) â†’
`tool_result` â†’ `agent_choice` (final response). The gateway must read the
**entire stream** to collect all `agent_choice` content.

### 7.4 Volume Mounts

| Mount | Type | Container Path | Purpose |
|---|---|---|---|
| `~/.mobyclaw/` | Bind mount | `/home/agent/.mobyclaw` | All agent state: memory, soul, sessions, logs |
| Agent config | Bind mount (ro) | `/agent/` | Agent YAML (from repo) |

**Key principle:** Everything lives at `~/.mobyclaw/` on the host. No Docker
volumes. This means:
- All state persists across container restarts
- `cp -r ~/.mobyclaw/ backup/` is a complete backup
- `docker system prune` won't destroy anything

### 7.4 Secrets & Environment Variables

All secrets and configuration live in a **single `.env` file** at the project
root. Docker Compose loads it via `env_file` and injects variables into the
right containers.

#### Strategy

- **One `.env` file** â€” single place for all secrets. No scattered config.
- **`.env.example`** â€” checked into git with placeholder values. Users copy to
  `.env` and fill in their keys.
- **`.env` is gitignored** â€” never committed. `.gitignore` includes `.env` from
  day one.
- **No secrets baked into images** â€” the Dockerfile never `COPY`s `.env` or
  `ARG`s secrets. They're injected at runtime via Compose.
- **Least-privilege distribution** â€” each container only receives the env vars
  it needs. The agent container gets LLM API keys. The gateway gets messaging
  tokens. Neither gets the other's secrets.

#### Why `.env` file (not Docker Secrets, Vault, etc.)

Mobyclaw is a **personal agent on your own machine**. Docker Secrets requires
Swarm mode. Vault/SOPS/etc. add operational complexity for zero benefit when
you're the only user. A `.env` file is:
- Simple: one file, `cp .env.example .env`, edit, done
- Standard: Docker Compose native support, every dev knows it
- Portable: copy `.env` to a new machine alongside `~/.mobyclaw/`
- Secure enough: file permissions (`chmod 600 .env`), gitignored, never in images

If someone deploys mobyclaw on a shared server or CI, they can use their
platform's native secret injection (GitHub Actions secrets, systemd credentials,
etc.) â€” those just set env vars, which Compose picks up the same way.

#### Variable Reference

| Variable | Container | Required | Purpose |
|---|---|---|---|
| `ANTHROPIC_API_KEY` | moby | Yes (if using Anthropic) | Anthropic model access |
| `OPENAI_API_KEY` | moby | Yes (if using OpenAI) | OpenAI model access |
| `TELEGRAM_BOT_TOKEN` | gateway | No | Enables Telegram adapter |
| `DISCORD_BOT_TOKEN` | gateway | No | Enables Discord adapter |
| `SLACK_BOT_TOKEN` | gateway | No | Enables Slack adapter |
| `WHATSAPP_AUTH` | gateway | No | Enables WhatsApp adapter |
| `MOBYCLAW_HEARTBEAT_INTERVAL` | gateway | No | Heartbeat frequency (default: `15m`) |
| `MOBYCLAW_ACTIVE_HOURS` | gateway | No | Heartbeat window (default: `07:00-23:00`) |
| `MOBYCLAW_HOME` | all | No | Override `~/.mobyclaw/` path |

**Convention:** Messaging adapter tokens double as feature flags â€” if
`TELEGRAM_BOT_TOKEN` is unset, the Telegram adapter simply doesn't load.
No token = no adapter = no error.

#### Least-Privilege Distribution in Compose

```yaml
services:
  moby:
    environment:
      - ANTHROPIC_API_KEY         # LLM keys only
      - OPENAI_API_KEY
    # NO messaging tokens

  gateway:
    environment:
      - TELEGRAM_BOT_TOKEN        # Messaging tokens only
      - DISCORD_BOT_TOKEN
      - SLACK_BOT_TOKEN
      - WHATSAPP_AUTH
      - MOBYCLAW_HEARTBEAT_INTERVAL
    # NO LLM API keys
```

The `.env` file holds everything, but Compose's per-service `environment`
block controls which container sees which variable. This way, a compromised
gateway can't leak your Anthropic key, and a compromised agent can't access
your Telegram bot.

#### `.env.example` Template

```bash
# â”€â”€â”€ LLM Provider Keys â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# At least one is required. Uncomment and fill in.
ANTHROPIC_API_KEY=
# OPENAI_API_KEY=

# â”€â”€â”€ Messaging (all optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Set a token to enable that channel. No token = adapter disabled.
# TELEGRAM_BOT_TOKEN=
# DISCORD_BOT_TOKEN=
# SLACK_BOT_TOKEN=
# WHATSAPP_AUTH=

# â”€â”€â”€ Agent Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MOBYCLAW_HOME=~/.mobyclaw
# MOBYCLAW_HEARTBEAT_INTERVAL=30m
```

#### File Permissions

`mobyclaw init` sets `chmod 600 .env` after creating it. The `.env` file
contains API keys worth money â€” it should only be readable by the owner.

---

## 8. Docker Compose Design

### 8.1 Phase 1 â€” Agent Only (no gateway)

```yaml
services:
  moby:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - ./agents/moby:/agent:ro
      - ${MOBYCLAW_HOME:-~/.mobyclaw}:/home/agent/.mobyclaw
    env_file:
      - .env
    restart: unless-stopped
    networks:
      - mobyclaw
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "1.0"

networks:
  mobyclaw:
    driver: bridge
```

### 8.3 Compose Override Generation

User-specific configuration (credentials and workspaces) is injected via
`docker-compose.override.yml`, which is **auto-generated** by the CLI on
every `mobyclaw up`. This file is gitignored and treated as a derived
artifact â€” never hand-edited.

**Mechanism:**

```
mobyclaw up
  â”‚
  â”œâ”€ Read ~/.mobyclaw/credentials.env
  â”œâ”€ Read ~/.mobyclaw/workspaces.conf
  â”œâ”€ Generate docker-compose.override.yml
  â”‚     â”œâ”€ env_file: for credentials (if any key=value lines exist)
  â”‚     â””â”€ volumes:  for workspaces (if any name=path lines exist)
  â””â”€ docker compose up (picks up override automatically)
```

**Generated override example:**

```yaml
# AUTO-GENERATED by mobyclaw â€” do not edit manually
services:
  moby:
    env_file:
      - /Users/you/.mobyclaw/credentials.env
    volumes:
      - /Users/you/projects/myapp:/workspace/myapp
      - /Users/you/Documents/notes:/workspace/notes
```

**Design decisions:**
- **Override, not inline in docker-compose.yml** â€” The base compose file stays
  static and git-committed. Per-user config lives in the override. Docker
  Compose merges them automatically.
- **Regenerated every time** â€” The override is rebuilt from `credentials.env`
  and `workspaces.conf` on each `up`. This means edits to those config files
  take effect immediately on next restart.
- **Graceful degradation** â€” If both files are empty/missing/comment-only,
  no override is generated and the base compose works as-is.
- **Absolute paths** â€” The override uses absolute paths to `credentials.env`
  because Docker Compose resolves env_file paths relative to the compose file
  location, not the user's home.

**Why `docker-compose.override.yml` (not `-f` flag)?**
Docker Compose automatically loads `docker-compose.override.yml` when it
exists in the same directory. No need for extra `-f` flags. The CLI's
`docker compose -f docker-compose.yml` still picks it up.

Phase 1 is **one container**. moby has `~/.mobyclaw/` bind-mounted and uses
cagent's built-in filesystem tools to read/write memory directly. No separate
memory or workspace services yet.

### 8.2 Phase 2 â€” Full Stack with Gateway

```yaml
services:
  moby:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - ./agents/moby:/agent:ro
      - ${MOBYCLAW_HOME:-~/.mobyclaw}:/home/agent/.mobyclaw
    env_file:
      - .env
    environment:
      - ANTHROPIC_API_KEY         # LLM keys only
      - OPENAI_API_KEY
    restart: unless-stopped
    networks:
      - mobyclaw

  gateway:
    build:
      context: ./gateway
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    volumes:
      - ${MOBYCLAW_HOME:-~/.mobyclaw}:/data/.mobyclaw
    env_file:
      - .env
    environment:
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN:-}
      - DISCORD_BOT_TOKEN=${DISCORD_BOT_TOKEN:-}
      - SLACK_BOT_TOKEN=${SLACK_BOT_TOKEN:-}
      - WHATSAPP_AUTH=${WHATSAPP_AUTH:-}
      - MOBYCLAW_HEARTBEAT_INTERVAL=${MOBYCLAW_HEARTBEAT_INTERVAL:-30m}
    depends_on:
      - moby
    restart: unless-stopped
    networks:
      - mobyclaw

networks:
  mobyclaw:
    driver: bridge
```

---

## 9. CLI Design (`mobyclaw`)

A **bash script** at `./mobyclaw` that wraps Docker Compose with agent-aware
commands. It's the primary interface for setting up, running, and interacting
with moby.

### Commands

| Command | What it does | Phase |
|---|---|---|
| `mobyclaw init` | **Interactive setup** â€” LLM, channels, agent config | 1 |
| `mobyclaw up` | Start Moby (runs init automatically if needed) | 1 |
| `mobyclaw down` | Stop everything | 1 |
| `mobyclaw logs [service]` | Tail logs | 1 |
| `mobyclaw status` | Show running services, connected channels, agent health | 1 |
| `mobyclaw run "<prompt>"` | Send a one-shot prompt to moby via HTTP | 1 |
| `mobyclaw chat` | Interactive chat session with moby (CLI) | 1 |
| `mobyclaw exec` | Shell into the agent container | 1 |
| `mobyclaw workspace list` | Show mounted workspaces | 1 |
| `mobyclaw workspace add <path> [name]` | Mount a host folder | 1 |
| `mobyclaw workspace remove <name>` | Unmount a folder | 1 |
| `mobyclaw help` | Show help | 1 |
| `mobyclaw version` | Show version | 1 |
| `mobyclaw memory` | Show recent memory entries | 2 |
| `mobyclaw cron list` | Show scheduled cron jobs | 2 |
| `mobyclaw cron add` | Add a cron job | 2 |
| `mobyclaw channels` | Show connected messaging channels | 2 |

### `mobyclaw init` â€” Interactive Onboarding

The init command is the entry point for new users. It walks through setup
interactively, asking only what's needed and skipping everything else.

**Flow:**

```
mobyclaw init
  â”‚
  â”œâ”€ 1. Check prerequisites (docker, curl)
  â”‚     â””â”€ Fail fast if missing
  â”‚
  â”œâ”€ 2. LLM Provider
  â”‚     â”œâ”€ Choose: Anthropic / OpenAI / Both
  â”‚     â”œâ”€ Enter API key(s) (hidden input)
  â”‚     â””â”€ Choose model (sensible default offered)
  â”‚
  â”œâ”€ 3. Messaging Channels (all optional, default: skip)
  â”‚     â”œâ”€ Telegram?  â†’ token or skip
  â”‚     â”œâ”€ Discord?   â†’ token or skip
  â”‚     â”œâ”€ Slack?     â†’ token or skip
  â”‚     â””â”€ WhatsApp?  â†’ auth or skip
  â”‚
  â”œâ”€ 4. Service Credentials (all optional, default: skip)
  â”‚     â”œâ”€ GitHub?  â†’ GH_TOKEN or skip
  â”‚     â”œâ”€ AWS?     â†’ key pair or skip
  â”‚     â””â”€ Custom?  â†’ name=value loop
  â”‚
  â”œâ”€ 5. Workspace Folders (all optional, default: skip)
  â”‚     â””â”€ Add folder?  â†’ path + name loop
  â”‚
  â”œâ”€ 6. Agent Settings
  â”‚     â”œâ”€ Heartbeat interval (default: 30m)
  â”‚     â””â”€ Data directory (default: ~/.mobyclaw)
  â”‚
  â”œâ”€ 7. Create data directory
  â”‚     â”œâ”€ ~/.mobyclaw/{memory,sessions,logs}/
  â”‚     â”œâ”€ Copy soul.yaml (if not exists â€” never overwrite)
  â”‚     â”œâ”€ Copy MEMORY.md (if not exists)
  â”‚     â”œâ”€ Copy HEARTBEAT.md (if not exists)
  â”‚     â”œâ”€ Write credentials.env (append new, keep existing)
  â”‚     â””â”€ Write workspaces.conf (append new, keep existing)
  â”‚
  â”œâ”€ 8. Write .env file
  â”‚     â”œâ”€ All config in one file
  â”‚     â”œâ”€ chmod 600 (secrets protection)
  â”‚     â””â”€ Commented-out lines for skipped services
  â”‚
  â”œâ”€ 9. Generate docker-compose.override.yml
  â”‚     â”œâ”€ env_file for credentials (if any)
  â”‚     â””â”€ volumes for workspaces (if any)
  â”‚
  â””â”€ 10. Summary + next steps
        â”œâ”€ What was configured
        â”œâ”€ What files were created
        â””â”€ "Run: mobyclaw up"
```

**Design principles for init:**

- **Skip by default** â€” Messaging channels default to "no". Only LLM is
  required. This means a user can get running with just an API key.
- **Never overwrite user data** â€” If `~/.mobyclaw/soul.yaml` already exists,
  init keeps it. Re-running init is safe.
- **Re-runnable** â€” Running `init` again asks if you want to overwrite `.env`.
  Useful for adding a new channel or changing providers.
- **Hidden input for secrets** â€” API keys use `read -s` (no echo). Never
  shown on screen.
- **Sensible defaults everywhere** â€” Enter through the whole flow and you
  get a working setup with Anthropic Claude.
- **Guidance inline** â€” Each channel prompt includes a brief hint on where
  to get the token (BotFather link, Discord dev portal, etc.).

### Design Decisions

- **Bash script, not a compiled binary** â€” Keep it simple. Docker + curl + jq
  are the only dependencies.
- **Thin wrapper over docker compose** â€” CLI adds agent-awareness but delegates
  all container lifecycle to Compose.
- **`mobyclaw run` uses HTTP** â€” Sends a prompt to cagent's API via curl.
- **`mobyclaw chat` for interactive** â€” Opens a streaming conversation loop.
- **`mobyclaw up` auto-inits** â€” Running `up` without prior init seamlessly
  runs the full init flow, then immediately starts containers. One command
  from zero to running agent. `init` still exists as a standalone command
  for users who want to configure without starting.
- **`mobyclaw init` is interactive, not flag-based** â€” A personal agent setup
  is a one-time event. Interactive prompts are friendlier than `--flag` soup.
  Power users can skip init entirely and just write `.env` manually.

---

## 10. Agent Loop (Powered by cagent)

We do NOT implement our own agent loop. cagent handles the full cycle:

```
Prompt (from gateway, CLI, or scheduler)
  â”‚
  â–¼
cagent serve api
  â”‚
  â”œâ”€ Assembles system prompt (soul.yaml instruction + context)
  â”œâ”€ Model inference (Anthropic/OpenAI/etc.)
  â”œâ”€ Tool execution (shell, filesystem, fetch, etc.)
  â”‚   â”œâ”€ Read MEMORY.md, memory/*.md
  â”‚   â”œâ”€ Write new memories
  â”‚   â”œâ”€ Execute shell commands
  â”‚   â”œâ”€ Tool results fed back to model
  â”‚   â””â”€ Loop until model produces final response
  â”œâ”€ Response streaming
  â””â”€ Session persistence (managed by cagent)
```

**Design decision:** Delegating the agent loop entirely to cagent means:
- We get tool execution, streaming, retries, context management for free
- We focus on what matters: orchestration, messaging, and memory
- Upgrades to cagent automatically improve all mobyclaw agents

---

## 11. Security Model

### Phase 1 (Simple)

| Concern | Mitigation |
|---|---|
| Agent isolation | Runs in its own container |
| Workspace access | Volume mounts control what agent can see |
| API key exposure | `.env` file, not baked into images; least-privilege per container (Â§7.4) |
| Network access | Agent can reach internet (needed for LLM APIs) |
| Resource limits | Compose `deploy.resources` caps memory + CPU |
| Host access | Non-root container user, no privileged mode |

### Phase 2 (Hardened)

- Read-only root filesystem with tmpfs for `/tmp`
- Network policy: agent can only reach LLM APIs + gateway
- DM access control: allowlists per messaging channel
- Agent-specific API key scoping
- Workspace access tiers: `none`, `ro`, `rw`

---

## 12. Phased Roadmap

### Phase 1 â€” Agent in a Box âœ¦ START HERE

**Goal:** Run moby in a Docker container with persistent memory, interact via CLI.

Deliverables:
1. `agents/moby/soul.yaml` â€” Moby's personality, model, tools, behavior (all-in-one)
3. `Dockerfile` â€” Agent base image with cagent
4. `docker-compose.yml` â€” Single-service compose (moby only)
5. `mobyclaw` â€” CLI script (up, down, logs, status, run, chat)
6. `.env.example` â€” API key template
7. `README.md` â€” Getting started guide

Success criteria:
- `mobyclaw up` starts moby in a container (long-lived, always running)
- `mobyclaw up` on a fresh machine walks through interactive setup first
- `mobyclaw run "Hello, who are you?"` gets a personality-rich response
- `mobyclaw run "Remember that my name is Alice"` â†’ agent writes to `~/.mobyclaw/MEMORY.md`
- `mobyclaw run "What's my name?"` â†’ agent recalls from MEMORY.md
- `mobyclaw chat` opens an interactive session
- Memory persists across `mobyclaw down && mobyclaw up`

### Phase 2 â€” Gateway + Messaging

**Goal:** Chat with moby through Telegram. Heartbeat and cron working.

Deliverables:
- `gateway/` â€” Gateway container (message routing, sessions, scheduler)
- Messaging adapters (Telegram first, then WhatsApp, Discord, Slack)
- Heartbeat system (periodic agent wake-ups)
- Cron job system (scheduled tasks)
- Session management (conversation threads per user/channel)
- Updated docker-compose.yml with gateway service
- `mobyclaw channels` and `mobyclaw cron` CLI commands

Success criteria:
- Send a Telegram message â†’ get a response from moby
- Moby remembers conversations across Telegram messages
- Heartbeat fires every 30m, moby checks HEARTBEAT.md
- Cron job sends a daily morning summary to Telegram
- `mobyclaw status` shows connected channels

### Phase 3 â€” Workspace + Integrations

**Goal:** Agent can access local files. More messaging channels.

Deliverables:
- Workspace service (local filesystem mounts, MCP server)
- More messaging adapters in gateway (WhatsApp, Discord, Slack)
- Vector memory search (semantic recall over memory files)
- Webhook ingress (GitHub events, etc.)
- Web UI for management and chat

### Phase 4 â€” Production Hardening

**Goal:** Ready for real 24/7 workloads.

Deliverables:
- Security hardening (seccomp, read-only root, network policy)
- Monitoring and observability (logs, metrics, health checks)
- Session compaction (summarize old context)
- Auto memory flush before compaction
- Plugin/skill system


---

## 13. Key Architectural Decisions Log

| # | Decision | Rationale | Date |
|---|---|---|---|
| ADR-001 | Use cagent native YAML, no wrapper format | Zero translation layer, users get full cagent features | 2026-02-23 |
| ADR-002 | `soul.yaml` as single identity file per agent | Simpler than OpenClaw's 6+ bootstrap files. Can add more via `add_prompt_files` | 2026-02-23 |
| ADR-003 | `cagent serve api` as primary container entrypoint | HTTP API is the natural interface for containerized agents | 2026-02-23 |
| ADR-004 | Bash CLI, not compiled binary | Minimal dependencies (docker, curl, jq). Ship fast, iterate. | 2026-02-23 |
| ADR-005 | Debian slim base image | Better cagent/tool compat than Alpine. Acceptable size trade-off. | 2026-02-23 |
| ADR-006 | `mobyclaw.yaml` is dev-only, not product config | Separation of concerns: dev agent â‰  product agent | 2026-02-23 |
| ADR-007 | "moby" as the default/reference agent | Clear identity, easy onboarding, extensible pattern | 2026-02-23 |
| ADR-008 | Docker Compose over Kubernetes | Right-sized for personal agent deployment. K8s is overkill. | 2026-02-23 |
| ADR-009 | Delegate agent loop entirely to cagent | Focus on orchestration, not reimplementing inference + tool execution | 2026-02-23 |
| ADR-010 | Memory as plain Markdown files (OpenClaw pattern) | Simple, portable, agent can read/write with filesystem tools. No DB needed. | 2026-02-23 |
| ADR-011 | Gateway as separate container from agent | Clean separation: gateway handles I/O + routing, agent handles thinking + acting | 2026-02-23 |
| ADR-012 | Messaging adapters inside gateway, not separate containers | Simpler (one container), all JS libs anyway, enable/disable via env vars. Matches OpenClaw. | 2026-02-23 |
| ADR-013 | Docker volumes for persistence | Workspace (memory) and data (sessions, cron) survive container restarts | 2026-02-23 |
| ADR-014 | 4-service separation: moby, gateway, workspace, memory | Each concern in its own container. Clean ownership. Independent scaling/failure. | 2026-02-23 |
| ADR-015 | Workspace + memory as MCP servers | cagent's `type: mcp` toolset connects moby to services. No direct host mounts on agent. | 2026-02-23 |
| ADR-016 | Separate workspace and memory volumes | Workspace = host files (projects, code). Memory = agent state (MEMORY.md, daily logs). Different lifecycles, different owners. | 2026-02-23 |
| ADR-017 | `~/.mobyclaw/` as user data directory, bind-mounted | User-visible, editable, portable, survives `docker system prune`. Not a Docker volume. | 2026-02-23 |
| ADR-018 | Messaging adapters inside gateway, not separate bridge containers | Simpler, less config, matches OpenClaw. Enable via env var presence. | 2026-02-23 |
| ADR-019 | Single agent only â€” no multi-agent support | Mobyclaw is a personal agent, not a platform. One agent (moby), one container. Simplifies routing, config, and mental model. Can always revisit. | 2026-02-23 |
| ADR-020 | Sessions created with `tools_approved: true` | `cagent serve api` pauses at `tool_call_confirmation` unless the session has `tools_approved: true`. Gateway sets this on session creation. Container isolation provides the safety boundary. | 2026-02-23 |
| ADR-021 | `.env` file for secrets management | Single file, Docker Compose native, no Swarm/Vault needed. Least-privilege: per-service `environment` blocks control which container sees which var. | 2026-02-23 |
| ADR-022 | End-to-end streaming via SSE PassThrough | cagent emits tokens in real-time. Gateway streams them through via PassThrough piped to HTTP response. Critical: use `res.on('close')` not `req.on('close')` for disconnect detection. Telegram adapter edits message every ~1s. CLI prints tokens to stdout. | 2026-02-23 |
| ADR-023 | `docker-compose.override.yml` for per-user config | Base compose stays static + git-committed. Override is auto-generated from `credentials.env` + `workspaces.conf` on every `mobyclaw up`. Docker Compose merges them automatically. Gitignored. | 2026-02-23 |
| ADR-024 | Separate `credentials.env` from `.env` | `.env` = mobyclaw infra (LLM keys, messaging). `credentials.env` = user service tokens (gh, aws). Different owners, different lifecycle. credentials.env lives in `~/.mobyclaw/` (portable with agent state). | 2026-02-23 |
| ADR-025 | Workspaces as host bind mounts via `workspaces.conf` | Simple `name=path` format in `~/.mobyclaw/workspaces.conf`. CLI manages it (`workspace add/remove/list`). Override generation maps to Docker volumes. Changes require restart. | 2026-02-23 |
| ADR-026 | Gateway-side scheduler with agent-created schedules via REST API | Agent calls `POST /api/schedules` via curl. Gateway owns timing, persistence, and delivery. Separation: agent composes messages, gateway delivers at the right time. No agent involvement at fire time (pre-composed messages). | 2026-02-23 |
| ADR-027 | Heartbeat as periodic agent prompt, separate from scheduler | Scheduler = precise dumb timer (30s resolution). Heartbeat = intelligent agent review (15m interval). Different concerns: scheduler delivers pre-composed messages; heartbeat invokes full LLM reasoning. Agent uses `/api/deliver` to proactively message users from heartbeat. | 2026-02-23 |
| ADR-028 | TASKS.md as agent-managed task store (Markdown) | Flexible Markdown file. Agent writes entries via filesystem tools. `[scheduled]` marker prevents double-scheduling. Channel stored per-task. Heartbeat reviews it. Complements schedules.json (gateway-owned) â€” TASKS.md is the agent's view, schedules.json is the gateway's execution state. | 2026-02-23 |
| ADR-029 | Channel context injected as message prefix by gateway | Gateway prepends `[context: channel=telegram:123, time=...]` to every user message. Only mechanism available since cagent API has no per-message metadata. Agent extracts channel for schedule creation. Never displayed to user. | 2026-02-23 |
| ADR-030 | Last active channel for fallback delivery | Gateway tracks last messaging channel used. Fallback when heartbeat/agent needs to deliver without a specific channel target. Resets on restart (acceptable for personal agent). | 2026-02-23 |

---

## 14. Open Questions

- ~~**cagent serve api exact endpoints**~~: **RESOLVED** â€” See Â§7.3.
- ~~**cagent session management**~~: **RESOLVED** â€” cagent manages sessions natively.
  Gateway only needs to track channelId â†’ sessionId mapping.
- ~~**Gateway language**~~: **RESOLVED** â€” Node.js (JavaScript). Telegraf, express,
  and other messaging libraries are all JS. Works well.
- ~~**Health checks**~~: **RESOLVED** â€” `GET /api/ping` returns `{"status":"ok"}`.
  Used in Dockerfile HEALTHCHECK and gateway's `waitForReady()`.
- **MCP stdio over network**: cagent's MCP toolset uses `command` (stdio transport).
  For workspace/memory in separate containers, we need a thin CLI client that
  bridges stdio â†” network (e.g., `mcp-client http://workspace:9100`). Need to
  build or find this.
- **Memory search**: Phase 2+ needs vector search over memory files. Options:
  embedded SQLite with vector extension, or a lightweight sidecar (Qdrant, Chroma).
- **Hot reload**: Can we update `soul.yaml` without restarting the agent container?
  cagent may re-read instructions on each request.
- **Heartbeat in Phase 1**: Could we do a lightweight heartbeat via a simple
  cron/timer on the host that curls the agent API? This would give us heartbeat
  behavior even before the gateway exists.

---

*Last updated: 2026-02-23*
